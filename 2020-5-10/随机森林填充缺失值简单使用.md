# 			随机森林填充缺失值简单使用

​	首先我们简单介绍下决策树，想象一下，一个名叫安德鲁的人，想知道一年的假期旅行中他应该去哪些地方。他会向了解他的朋友们咨询建议。

​	起初，他去寻找一位朋友，这位朋友会问安德鲁他曾经去过哪些地方，他喜欢还是不喜欢这些地方。基于这些回答就能给安德鲁一些建议，这便是一种典型的决策树算法。

​	朋友通过安德鲁的回答，为其制定出一些规则来指导应当推荐的地方。随后，安德鲁开始寻求越来越多朋友们的建议，他们会问他不同的问题，并从中给出一些建议。 最后，安德鲁选择了推荐最多的地方，这便是典型的随机森林算法。

​	具体的原理就不细讲，主要讲讲怎么运用。



在处理特征矩阵时，往往各列都会存在缺失值，寻找数据时难免遇到缺失，而我们若将缺少的行直接舍弃，往往会增大噪音，对结果产生不可逆的影响。因此我们需要一种方法将缺少的数据填充起来。

​	其中随机森林回归可有效填充缺失值。比如说，在一个“用地区，环境，附近学校数量”预测“房价”的问题中，我们既可以用“地区”，“环境”，“附近学校数量”的数据来预测“房价”，也可以反过来，用“环境”，“附近学校数量”和“房价”来预测“地区”。而回归填补缺失值，正是利用了这种思想。

​	 一般随机森林填充缺失值，我们假设为T，我们需要其他n-1列特征，且这些特征必须是无缺少值的。而现实中，我们往往是多列都有缺失值，对于这种情况，我们可以遍历所有特征，从特征最少的一列开始填充，填充时，将其他特征列的缺少值用0填充，每完成一次回归预测，就将预测值放到原本的特征矩阵中，再继续填补下一个特征。每一次填补完毕，有缺失值的特征会减少一个，所以每次循环后，需要用0来填补的特征就越来越少。遍历完所有后，理论上就不再有缺失值了。

​	随机森林的主要优点是：

1. 处理大量的多达几千个的自变量。意味者特征矩阵存在大量特征时，也可以拥有恰当的鲁棒性。

2. 随机森林对离群值不敏感，在随机干扰较多的情况下表现稳健。

3. 现有的随机森林算法评估所有变量的重要性，而不需要顾虑一般回归问题面临的多元共线性的问题。此意味这选取一个特征后再选取与这特征紧密相关的另一特征，预测结果差不多。

   网上扒的代码：

   

   

   ```python
   def fill_missing_rf(X,y,to_fill):
   """
   使用随机森林填补一个特征的缺失值的函数
   
   参数：
   X：要填补的特征矩阵
   y：完整的，没有缺失值的标签
   to_fill：字符串，要填补的那一列的名称
   """
   
   #构建我们的新特征矩阵和新标签
   df = X.copy()
   fill = df.loc[:,to_fill]
   df = pd.concat([df.loc[:,df.columns != to_fill],pd.DataFrame(y)],axis=1)
   
   # 找出我们的训练集和测试集
   Ytrain = fill[fill.notnull()]#特征不缺失的值
   Ytest = fill[fill.isnull()]#特征缺失的值
   Xtrain = df.iloc[Ytrain.index,:]#特征不缺失的值对应其他n-1个特征+本来的标签
   Xtest = df.iloc[Ytest.index,:]#特征缺失的值对应其他n-1个特征+本来的标签
   
   #用随机森林回归来填补缺失值
   from sklearn.ensemble import RandomForestRegressor as rfr
   rfr = rfr(n_estimators=100)
   rfr = rfr.fit(Xtrain, Ytrain)
   Ypredict = rfr.predict(Xtest)
   
   return Ypredict
   ```

```python
X = data.iloc[:,1:]
y = data["SeriousDlqin2yrs"]#y = data.iloc[:,0]
X.shape#(149391, 10)

#=====[TIME WARNING:1 min]=====#
y_pred = fill_missing_rf(X,y,"MonthlyIncome")

#注意可以通过以下代码检验数据是否数量相同
# y_pred.shape ==  data.loc[data.loc[:,"MonthlyIncome"].isnull(),"MonthlyIncome"].shape

#确认我们的结果合理之后，我们就可以将数据覆盖了
data.loc[data.loc[:,"MonthlyIncome"].isnull(),"MonthlyIncome"] = y_pred

data.info()

```

若多个特征值缺少，![在这里插入图片描述](https://img-blog.csdnimg.cn/20190901172435704.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0t5cmllX0lydmluZw==,size_16,color_FFFFFF,t_70)





决策树特征矩阵如下

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190302165306803.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0Ryb2tlX1pob3U=,size_16,color_FFFFFF,t_70#pic_center)

根据各个特征进行划分，求出各个结点的熵，随后算出信息增益，至此决策树建立完毕。

![img](https://img-blog.csdnimg.cn/2019030216533483.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0Ryb2tlX1pob3U=,size_16,color_FFFFFF,t_70#pic_center)